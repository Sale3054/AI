{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Spring 2018:  Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the pseudocode that we came up with for the **N-armed bandit** in-class notebook (Friday 20 April 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_smarter_training(n_train, actions, p_levers):\n",
    "    \n",
    "    assert n_train >= 100, 'Need at least 100 training episodes'\n",
    "    \n",
    "    Q = defaultdict(float)       # re-initializing\n",
    "    Nsa = defaultdict(int)\n",
    "    Nwin = defaultdict(int)\n",
    "    levers = []                  # not actually necessary to keep track of\n",
    "    rewards = []                 # to calculate running average reward\n",
    "    avg_reward = []              # running average reward\n",
    "    \n",
    "    for k in range(n_train):\n",
    "        # your code goes here!   # pick a lever to pull\n",
    "        # your code goes here!   # what is the result of that pull?\n",
    "        # your code goes here!   # update Nsa\n",
    "        # your code goes here!   # update Nwin\n",
    "        # your code goes here!   # update Q\n",
    "\n",
    "        # HERE is where the Q-learning, or the TD or ADP \n",
    "        # equations would go\n",
    "\n",
    "        # Solution:\n",
    "        eps = 1./(1+k)  # just one option. there are many that will work!\n",
    "        levers.append(epsilon_greedy_action(actions, Q, eps))   # pick a lever to pull\n",
    "        rewards.append(result(levers[-1], p_levers))   # what is the result of that pull?\n",
    "        Nsa[levers[-1]] += 1  # update Nsa\n",
    "        Nwin[levers[-1]] += rewards[-1]   # update Nwin\n",
    "        Q[levers[-1]] = Nwin[levers[-1]]/Nsa[levers[-1]]   # update Q\n",
    "\n",
    "        # If we are beyond the 100th training episode,\n",
    "        # then update the running average reward\n",
    "        if k >= 100:\n",
    "            avg_reward.append(np.mean(rewards[-100:]))\n",
    "\n",
    "    return Nsa, Nwin, Q, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That code is a bit simplified because we did not really have sequences of states, leading to a terminal state. Instead, with the N-armed bandit, every action led to a terminal state, where we received some reward.\n",
    "\n",
    "Here is some more general reinforcement learning pseudocode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(mdp, n_trials):\n",
    "\n",
    "    # initialize whatever you need\n",
    "    # Q-learning:   Q, a dictionary for state-action pairs;\n",
    "    #               initial policy, to be improved\n",
    "    # TD-learning:  Utilities\n",
    "    # ADP-learning: Transition probabilities, and counters\n",
    "    #               for the states/actions/subsequent states\n",
    "    # A learning rate parameter, maybe with a functional form\n",
    "        \n",
    "    for k in range(n_trials):\n",
    "        \n",
    "        # sample an initial state s (random, non-terminal)\n",
    "        # ... and its reward is also observed, r\n",
    "          \n",
    "        # run through a single training episode for n_steps\n",
    "        for t in range(n_steps):\n",
    "        # or run through until you hit a terminal state\n",
    "        while s not in terminal_states:\n",
    "\n",
    "            # if you are in a terminal state, then the\n",
    "            # policy is None, and the reward is known\n",
    "            # (you might update this elsewhere, or not at all,\n",
    "            #  but it depends on your specific implementation)\n",
    "            \n",
    "            # pick a new action, available from state s\n",
    "            \n",
    "            # sample a new state s' from transitions P(s'|s,a)\n",
    "            \n",
    "            # the percept that the agent senses is the\n",
    "            # new state and reward, s' and r'\n",
    "            \n",
    "            # ... update whatever your learning model is ...\n",
    "            # here is where the TD-learning equation would be, or\n",
    "            # the ADP update to the counters and transition estimate, or\n",
    "            # the Q-learning equation\n",
    "\n",
    "            # If doing active RL, update the policy\n",
    "            # --> what is the best action for this state?\n",
    "            #     (by our current estimate)\n",
    "            \n",
    "            # update for next iteration of this training episode:\n",
    "            #   s -> s'\n",
    "            #   r -> r'\n",
    "            \n",
    "    return [stuff]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

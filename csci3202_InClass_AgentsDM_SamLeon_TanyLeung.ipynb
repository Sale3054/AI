{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Spring 2018\n",
    "\n",
    "# Friday 2 February 2018\n",
    "\n",
    "# In-class notebook:  Search and decision-making\n",
    "\n",
    "<a id='top'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Your name(s): Sam Leon\n",
    "\n",
    "<br>\n",
    "\n",
    "* When you submit this to Moodle (under Quizlet 3), be sure to include all of your group members' names.\n",
    "* You may work in groups of up to 3 people,\n",
    "* but **all people** in the group must submit the assignment on their own Moodle account (because Moodle is a pain in the ass to create groups and this will still be faster than your normal quizlets).\n",
    "* There are two unrelated problems. It does not matter in which order you tackle them.\n",
    "  * Problem 1:  Roomba Agents.  We will code up an increasingly realistic Roomba AI. (Problem 0 is the set up)\n",
    "  * Problem 2:  Decision Analysis.  We will have a close look at a seemingly trivial decision.\n",
    "\n",
    "---\n",
    "\n",
    "Shortcuts:  [Top](#top) || [0](#p0) || [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) || [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) | [2f](#p2f) | [2g](#p2g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, load the packages below (you might find they are handy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 0\n",
    "\n",
    "<a/ id='p0'></a>\n",
    "\n",
    "#### Set up and representing the task environment\n",
    "\n",
    "Let's design some worlds! Specifically, we want to represent things like a brave knight searching a cave for treasure, or a puppy running around a park, or a Roomba cleaning up a messy house.\n",
    "\n",
    "This initial part (Problem 0) is going to have a lot of text. But I swear it is worth reading through, because the next few problems will build off of this generic task environment framework.\n",
    "\n",
    "**Final note:** Many of these problems are intentionally open-ended. Part of the point is to get some practice designing objects, specifically to represent things that are much more overtly \"AI\" than search algorithms. But (in my view) much more of the point is to **exercise the creative side of programming**. If there was only one answer, it wouldn't be nearly as much fun. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up some generic classes to represent `Environment`, `Agent` and `Thing`. We have a bunch of methods defined for an `Environment`, all based on what we imagine our needs might be for having an agent running around in the environment, picking things up (vacuuming, eating, holding,...) and putting them back down (a vacuum being full and spitting dirt back out, a puppy putting a bone down so it can drink water,...).\n",
    "\n",
    "The first 3 methods are used to set up the environment, and populate it with things, or remove things from it:\n",
    "* constructor: we feed in the dimensions of a square grid environment, and subtract 1 from each to account for the fact that Python is 0-based, but we are sending in the number of tiles in each direction. The lower-left corner of the environment is assumed to be (0,0). The environment can also have `things` and `agents` in it, so we create a list for each of those.\n",
    "* `add_thing`: we want to be able to add things to the environment. Later, we will look at creating a class to represent `Thing`s.  We need as input to this a Thing and its location (Cartesian coordinates).\n",
    "* `remove_thing`: we might need to remove some things from the environment. \n",
    "\n",
    "The next 3 methods are useful for representing agents' **sensors**. Namely, we are concerned about whether or not the agent can sense things at or near the agent.\n",
    "* `things_near`: we might be interested to know what are all the things in the tiles *adjacent* to the agent's location (as well as at the agent's location), so we return this as a list.\n",
    "* `things_at`: perhaps we have an agent that can only sense what is going on in its location (otherwise, the same as `things_near`)\n",
    "* `percepts`: by default in this generic `Environment` class, the agents will be able to perceive anything *near* them. We can override this later depending on if we think our agent should be able to sense more or less.\n",
    "\n",
    "We have 3 methods to represent the agent moving through a 2D rectangular grid environment, as well as executing a couple other generic **actions**.\n",
    "* `hit_wall`: we know the boundaries of the environment, so if the agent tries to move outside of them, we need to let it know that it has bumped into a wall\n",
    "* `move`: the agent tries to execute a move\n",
    "* `execute_action`: here is the meat and cheese of the agent's actuators. In this generic class, we allow the agent to move in any of the four cardinal directions, as well as pick things up (`Grab`) and set things down (`Drop`).\n",
    "\n",
    "Finally, we need a few more methods to define how the actual simulation for the task environment is to proceed.\n",
    "* `is_done`: returns True if there are no more \"living\" agents to simulate\n",
    "* `step`: executes a single time step of the environment: the agent senses the environment, and then acts. This method by default is assuming a single agent, but we could override this to make it more general. You will notice that the `step` method is where we print output to the screen to stay apprised of what is going on in our virtual world.\n",
    "* `driver`: this is the driver method, which takes as an argument the number of time steps to run (default is 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width-1    # relative to 0, so subtract 1\n",
    "        self.height = height-1\n",
    "        self.things = []\n",
    "        self.agents = []\n",
    "\n",
    "    #\n",
    "    # The following 2 methods allow us to add Things or remove Things from the environment\n",
    "    #\n",
    "    \n",
    "    def add_thing(self, thing, location):\n",
    "        '''Add a thing to the environment and set its location.\n",
    "        For convenience, if thing is an agent program we make a new agent for it.'''\n",
    "        # first, check if the desired location is in-bounds\n",
    "        out_of_bounds = (location[0] > self.width or location[0] < 0 or\n",
    "                         location[1] > self.height or location[1] < 0)\n",
    "        if out_of_bounds:\n",
    "            print('Warning: failed to add {} object at {}'.format(thing.__class__.__name__, location))\n",
    "            return\n",
    "\n",
    "        if not isinstance(thing, Thing):\n",
    "            thing = Agent(thing)\n",
    "        thing.location = location\n",
    "        self.things.append(thing)\n",
    "        if isinstance(thing, Agent):\n",
    "            self.agents.append(thing)\n",
    "\n",
    "    def remove_thing(self, thing):\n",
    "        if thing in self.things:\n",
    "            self.things.remove(thing)\n",
    "            return True\n",
    "        elif thing in self.agents:\n",
    "            self.agents.remove(thing)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #\n",
    "    # The next 3 methods set up the agent's perception of things around it (SENSORS)\n",
    "    #\n",
    "    \n",
    "    def things_near(self, location):\n",
    "        '''return all things around the given location'''\n",
    "        things = []\n",
    "        for thing in self.things:  \n",
    "            # look within a radius of 1 around the agent; on Cartesian grid, \n",
    "            # this will only return the Things at the adjacent squares\n",
    "            # NOTE: if you wanted to let the agent have great vision/better sensors, this\n",
    "            #       could be modified\n",
    "            if np.sqrt( (thing.location[0]-location[0])**2 + (thing.location[1]-location[1])**2) <= 1:\n",
    "                things.append(thing)\n",
    "        return things\n",
    "    \n",
    "    def things_at(self, location):\n",
    "        '''return all things at the given location'''\n",
    "        things = []\n",
    "        for thing in self.things:        \n",
    "            if thing.location==location:\n",
    "                things.append(thing)\n",
    "        return things\n",
    "    \n",
    "    def percepts(self, agent):\n",
    "        '''the agent can perceive things N/S/E/W of their location'''\n",
    "        return self.things_near(agent.location)\n",
    "\n",
    "    #\n",
    "    # The next 3 methods set up how the agent can interact with the environment (ACTUATORS)\n",
    "    #\n",
    "    \n",
    "    def hit_wall(self, location):\n",
    "        '''hit a wall/boundary if the agent tries to go out of bounds'''\n",
    "        return (location[0] > self.width or location[0] < 0 or location[1] > self.height or location[1] < 0)\n",
    "        \n",
    "    def move(self, agent, direction):\n",
    "        '''get the new agent location, but revert to old if it bumps into boundary'''\n",
    "        locx, locy = agent.location\n",
    "        if direction=='N':\n",
    "            newLocation = locx, locy+1    \n",
    "        elif direction=='S':\n",
    "            newLocation = locx, locy-1\n",
    "        elif direction=='E':\n",
    "            newLocation = locx+1, locy\n",
    "        elif direction=='W':\n",
    "            \n",
    "            newLocation = locx-1, locy\n",
    "        bump = self.hit_wall(newLocation)\n",
    "        agent.location = newLocation if not bump else location\n",
    "        return (not bump)\n",
    "            \n",
    "    def execute_action(self, agent, action):\n",
    "        if action in ['N','S','E','W']:\n",
    "            bump = self.move(agent, action)\n",
    "        elif action == 'Grab':\n",
    "            things = [thing for thing in self.things_at(agent.location)]\n",
    "            if things:\n",
    "                agent.holding.append(things[0])\n",
    "                self.remove_thing(things[0])\n",
    "        elif action == 'Drop':\n",
    "            if agent.holding:\n",
    "                dropped = agent.holding.pop()\n",
    "                self.add_thing(thing=dropped, location=agent.location)\n",
    "\n",
    "    #\n",
    "    # These last 3 methods set up how the simulation for the task environment will be run\n",
    "    #\n",
    "    \n",
    "    def is_done(self):\n",
    "        '''end the simulation if there are no living agents (or end of time steps)'''\n",
    "        return not any(agent.is_alive() for agent in self.agents)\n",
    "\n",
    "    def step(self, quiet):\n",
    "        '''run one time step of the environment; assumes a single agent'''\n",
    "        if not self.is_done():\n",
    "            # assuming a single agent here!\n",
    "            agent = self.agents[0]\n",
    "            action = agent.function(self.percepts(agent))\n",
    "            if not quiet:\n",
    "                print('Agent {} executes action {} at location {}'.format(agent.name, action, agent.location))\n",
    "            self.execute_action(agent, action)\n",
    "\n",
    "    def driver(self, n_steps=5, quiet=False):\n",
    "        '''a driver method to run the environment for n_steps time steps\n",
    "        quiet=True will suppress output to screen to update (helpful for long simulations)'''\n",
    "        for step in range(n_steps):\n",
    "            self.step(quiet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, you've made it this far. You deserve a reward.  Here's a picture of a baby sloth wearing pajamas and taking a selfie.\n",
    "\n",
    "<img src=\"https://i.pinimg.com/736x/bc/4b/4c/bc4b4c4c01a6a82d8991524c9f41f6ce--pajamas-pjs.jpg\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "Now then. We should define some generic classes for `Thing`s and `Agent`s.  We can consider an `Agent` to be a subclass of `Thing`, since it is just a special case of general objects that we are putting in our environment.\n",
    "\n",
    "Our generic `Thing` class is pretty sparse. Basically, we are going to throw down some objects and declare whether or not they are alive. It serves as a basis for representing other things later that we can add other useful attributes to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Thing:\n",
    "    '''represent things in the environment'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def is_alive(self):\n",
    "        '''is this thing alive?'''\n",
    "        return hasattr(self, 'alive') and self.alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create an `Agent` as a subclass of `Thing`.  We should provide a **name** for our agents, and an **agent function**.  Recall that the agent function is what is \"doing the AI\". That is, the agent function takes as input the percepts, and provides as output the appropriate agent actions to take.\n",
    "\n",
    "Note that in case an agent function argument is not provided, we have a nice default course of action for a listless agent. There is also an attribute of the agent to store its `performance` measure, once we decide how to evaluate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(Thing):\n",
    "    \n",
    "    def __init__(self, name, agent_function=None):\n",
    "        self.alive = True\n",
    "        self.name = name\n",
    "        self.holding = []\n",
    "        self.performance = 0\n",
    "        if agent_function is None:\n",
    "            print('Warning: Agent {} missing agent_function. Using a silly default.'.format(self.name))\n",
    "            \n",
    "            def agent_function(percepts):\n",
    "                '''Only move left. It is the finest direction, after all.\n",
    "                Note that this only is run if the user does not supply a valid agent_function'''\n",
    "                return 'W'\n",
    "\n",
    "        self.function = agent_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"http://www.irobotweb.com/-/media/Images/Product-Pages/Roomba-Learn/Mini-Compare/960-Product-Image.png?h=292&la=en&w=286\" alt=\"Drawing\" style=\"width: 120px;\"/>\n",
    "\n",
    "<a/ id='p1'></a>\n",
    "## Problem 1:  Let's talk Roombas.\n",
    "\n",
    "The generic `Environment`/`Thing`/`Agent` set-up above is a pretty nice framework for tackling a variety of problems, but we will need to override a few components of it to make this function well for the specific task environment of a Roomba cleaning up a dirty room.\n",
    "\n",
    "First, let's create a **simple reflex agent**, `ReflexRoombaAgent`, method.  This function will take in an argument **name**, representing the agent's name.  Then, an `agent_function` is defined.  This is the most problem-specific part.  The `agent_function` below is specific to our simple two-tile environment that we will play around with initially.  Later, we will modify and extend this to larger rooms, and to other problems.\n",
    "\n",
    "After defining the `agent_function`, the `ReflexRoombaAgent` constructs an `Agent` object using the generic template above, but with Roomba-specific programming (i.e., `agent_function`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReflexRoombaAgent(name):\n",
    "    '''reflex Roomba agent. Takes percept as input, which is a tuple of (location, status)\n",
    "    and returns the appropriate action. This will override the default in the Agent class of \n",
    "    only moving West'''\n",
    "    def agent_function(percepts):\n",
    "        location, status = percepts\n",
    "        if status == 'Dirty':\n",
    "            return 'Vacuum'\n",
    "        elif location == (0,0):\n",
    "            return 'E'\n",
    "        elif location == (1,0):\n",
    "            return 'W'\n",
    "        \n",
    "    return Agent(name, agent_function)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Roomba is going to need some `Thing`s to clean up - namely, dirt! So let's create a subclass of `Thing` to represent dirt. If you haven't seen or used the `pass` statement in Python, it is just a placeholder when there needs to be *some*thing in that line, since Python syntax relies on whitespace. (Otherwise, we wouldn't know when to end the definition for the Dirt class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dirt(Thing):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Roomba and Dirt, specific to our vacuum task environment, let's make the `Environment` a bit more problem-specific.  The agent interacts with the environment through percepts and actuators, so we will need to override those defaults from the main Environment class. \n",
    "\n",
    "For `percepts`, we will assume that the agent can sense (1) its location and (2) whether that location is clean or dirty.  `percepts` takes as the argument the **agent**, and returns a tuple of (**location**, **status**).  Note that the actual \"sensing\" is done by the `Environment` method `things_at(location)`. That line checks whether or not any of the `things_at` the agent's location are of class Dirt. Note that it is possible to have multiple instances of Dirt in  given location, which could reflect the fact that in real life, sometimes our house is just really gross.\n",
    "\n",
    "For actuators, the generic `execute_action` method is almost good enough. We can override the `Grab` action with something more meaningful to our application.  Let's rename that as `Vacuum`, and call the things we want to grab `messes`.\n",
    "\n",
    "We only want the Roomba to pick up things that are Dirt (ideally), so we add the condition `if isinstance(thing, Dirt)`.  Notice that the Roomba will only pick up the *first* unit of dirt that it senses. This reflects the reality that if a tile is *really* dirty, it should take the Roomba longer to clean it up than a just mildly dirty tile.\n",
    "\n",
    "Notice that we also store in Roomba's `holding` attribute all of the different units of dirt that it has picked up. Later, we will generalize this problem so that Roomba can only store a finite amount of dirt, which is of course how real vacuums work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VacuumEnvironment(Environment):\n",
    "            \n",
    "    def percepts(self, agent):\n",
    "        ''' the percept is a tuple of (location, status) '''\n",
    "        status = 'Dirty' if any([isinstance(item, Dirt) \n",
    "                                 for item in room.things_at(location=agent.location)]) else 'Clean'\n",
    "        return (agent.location, status)\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        ''' override the default execute_action in Environment class, so that\n",
    "        for the Roomba we remove '''\n",
    "        if action in ['N','S','E','W']:\n",
    "            bump = self.move(agent, action)\n",
    "        elif action == 'Vacuum':\n",
    "            messes = [thing for thing in room.things_at(roomba.location) if isinstance(thing, Dirt)]\n",
    "            if messes:\n",
    "                agent.holding.append(messes[0])\n",
    "                self.remove_thing(messes[0])\n",
    "        elif action == 'Drop':\n",
    "            if agent.holding:\n",
    "                dropped = agent.holding.pop()\n",
    "                self.add_thing(thing=dropped, location=agent.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1a'></a>\n",
    "### (1a)\n",
    "\n",
    "#### Enough chit chat. Play time!\n",
    "\n",
    "Create a **room** that is a `VacuumEnvironment` with width=2 and height=1 (i.e., the standard two-tile room from the introductory set of slides where we introduced the concept of agents). Notice that the Cartesian coordinates of the tiles are (0,0) and (1,0), for a width of 2 and height of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "room = VacuumEnvironment(width=2, height=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a room, and that room can get dirty, we should probably instantiate a `ReflexRoombaAgent`.  Since our `ReflexRoombaAgent` is of class `Agent`, which is a subclass of `Thing`, we will need to use the `add_thing` method to add our Roomba to the **room**.  Here, you can see that we are adding the Roomba to the tile at (0,0).\n",
    "\n",
    "**Most importantly:** Give your Roomba a sweet name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "roomba = ReflexRoombaAgent('Randy')\n",
    "room.add_thing(roomba, location=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to set up the environment, let's sprinkle some dirt around.  In fact, let's set it up so that both tiles are `Dirty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "room.add_thing(Dirt(), location=(0,0))\n",
    "room.add_thing(Dirt(), location=(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At long last, we are ready to actually run our simulation.  Let's run for 6 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Randy executes action Vacuum at location (0, 0)\n",
      "Agent Randy executes action E at location (0, 0)\n",
      "Agent Randy executes action Vacuum at location (1, 0)\n",
      "Agent Randy executes action W at location (1, 0)\n",
      "Agent Randy executes action E at location (0, 0)\n",
      "Agent Randy executes action W at location (1, 0)\n"
     ]
    }
   ],
   "source": [
    "room.driver(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that looks like it went smoothly.  We should check that everything is working though.  After 6 time steps, the Roomba should have been able to vacuum up the patches of dirt in both tiles.  Then, what does the agent do? Check out the `agent_function` definition to make sure that, after the agent vacuums up all the dirt, it does what you expect.\n",
    "\n",
    "We can also check to make sure the list of things in the room, and the list of what all the Roomba agent is holding, both match our expectations.  What should be left in the room?  What should Roomba be holding?  Design a couple `print` statements to check that our task environment looks the way we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Things left in the room: ['Agent']\n",
      "Things agent is holding: ['Dirt', 'Dirt']\n"
     ]
    }
   ],
   "source": [
    "print('Things left in the room: {}'.format([thing.__class__.__name__ for thing in room.things]))\n",
    "print('Things agent is holding: {}'.format([thing.__class__.__name__ for thing in roomba.holding]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that we have a basic Roomba agent in a very simple room, let's start to add in layers of complexity/reality. Each part of the rest of this problem adds another feature or set of features to make our Roomba Agents a bit more like actual Roombas.  By the end of this problem, our \"toy\" Roombas will be pretty similar to actual Roombas.\n",
    "\n",
    "For the next few parts, my approach would be to only modify the Roomba-specific programming from **Problem 1** and beyond; try to leave the stuff in **Problem 0** alone, if you can. The point of the `ReflexRoombaAgent` and `VacuumEnvironment` classes is to take the generic `Agent` and `Environment` classes and apply vacuum-specific methods, as opposed to hard-coding the entire gory mess from (e.g.) the `Environment` class in **Problem 0**. So, instead of jumping back and forth to modify the codes from **1a**, the easiest thing to do to implement these additional code features is probably to begin by copy-pasting, but then giving the new subclasses here new names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1b'></a>\n",
    "### (1b)\n",
    "\n",
    "Also implement the following features. It is probably easiest to do these **one at a time**, to make sure you know which breaks your code if things go off the rails.\n",
    "1. Include a performance measure for the agent to keep track of.  You should decide for yourselves how you want to measure your Roomba's performance. Here are some general guidelines:\n",
    "  * ~~Reward cleanliness~~\n",
    "  * ~~Penalize excessive moving around~~\n",
    "1. Implement a model-based agent\n",
    "  * Keep track of which tiles are clean/dirty\n",
    "  * Perhaps use an attribute of the agent called **model** that updates the agent's internal map of where the clean/dirty tiles all are. The entire room is initially unknown.\n",
    "  * For example, if the Roomba agent has just come from cleaning the tile at (0,0), and has now just cleaned the tile at (1,0), then the Roomba ought to know that going straight back to (0,0) is not the best option.\n",
    "  * You may want to add a **NoOp** (\"no operation\" or \"do nothing\") action choice, since moving around unnecessarily will worsen the agent's performance measure.\n",
    "1. Stochastically generate new dirt every so often\n",
    "  * Based on how long it has been since the agent last cleaned a particular tile, it may be necessary to circle back around and check the tiles again for dirt (since it may have appeared stochastically). Thus, the agent will need to track in its model *how long* it has been since it has cleaned each tile.\n",
    "  * It may be useful to print a message to the screen denoting the location of new dirt, if dirt appears somewhere.\n",
    "\n",
    "The following code snippet could implement stochastic dirt appearance with probability `p_dirt` for each tile for each time step.  You are encouraged to modify `p_dirt` as you see fit.  The default here is a 1/10 probability of dirt appearing on any given tile at any given time step, possibly from a puppy with muddy paws or a roommate just returning from a hike.  Note that *all* tiles will need to be updated in this way, and that `p_dirt` is probably most appropriate to specify as an attribute of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Thing):\n",
    "    \n",
    "    def __init__(self, name, agent_function=None):\n",
    "        self.alive = True\n",
    "        self.name = name\n",
    "        self.holding = []\n",
    "        self.performance = 0\n",
    "        self.clean_tiles = []\n",
    "        self.timer = 0\n",
    "        self.e_timer = 0\n",
    "        self.current_moves = []\n",
    "        self.walls = []\n",
    "        self.critical = False\n",
    "        self.criticalTimer = 0\n",
    "        if agent_function is None:\n",
    "            print('Warning: Agent {} missing agent_function. Using a silly default.'.format(self.name))\n",
    "            \n",
    "            def agent_function(percepts):\n",
    "                '''Only move left. It is the finest direction, after all.\n",
    "                Note that this only is run if the user does not supply a valid agent_function'''\n",
    "                return 'W'\n",
    "\n",
    "        self.function = agent_function\n",
    "        \n",
    "    def add_clean_tile(self, location):\n",
    "        self.clean_tiles.append(location)\n",
    "        \n",
    "    def remove_tiles(self, location):\n",
    "        if location in self.clean_tiles:\n",
    "            self.clean_tiles.remove(location)\n",
    "            \n",
    "        \n",
    "        \n",
    "def ModelRoombaAgent(name, agent):\n",
    "    '''reflex Roomba agent. Takes percept as input, which is a tuple of (location, status)\n",
    "    and returns the appropriate action. This will override the default in the Agent class of \n",
    "    only moving West'''\n",
    "    if agent == None:\n",
    "        return Agent(name, None)\n",
    "    \n",
    "    def agent_function(percepts):\n",
    "        location, status = percepts\n",
    "        '''Timer resets every 3 steps, so we recheck if tiles are clean'''\n",
    "        if agent.timer == 3: #Timer to keep track of how long it's been since we've check for cleanness\n",
    "            agent.clean_tiles = []\n",
    "            agent.timer = 0\n",
    "            #print('Wiping--remaining list:{}'.format(agent.clean_tiles))\n",
    "        agent.timer += 1\n",
    "        if status == 'Clean':\n",
    "            if location not in agent.clean_tiles:\n",
    "                #print('{} clean. Appending...{}'.format(location, agent.clean_tiles))\n",
    "                agent.clean_tiles.append(location)\n",
    "                agent.performance += 5\n",
    "        if status == 'Dirty':\n",
    "            agent.clean_tiles.append(location)\n",
    "            return 'Vacuum'\n",
    "        elif location == (0,0):\n",
    "            if (1,0) in agent.clean_tiles:\n",
    "                #print('Already visited (1,0), doing nothing!')\n",
    "                agent.performance += 5\n",
    "                return\n",
    "            #print('(1,0) not yet visited, moving East- {}'.format(agent.clean_tiles))\n",
    "            return 'E'\n",
    "        elif location == (1,0):\n",
    "            if (0,0) in agent.clean_tiles:\n",
    "                agent.performance += 5\n",
    "                #print('Already visited (1,0), doing nothing!')\n",
    "                return\n",
    "            #print('(0,0) not yet visited, moving West- {}'.format(agent.clean_tiles))\n",
    "            return 'W'\n",
    "    return Agent(name, agent_function)           \n",
    "\n",
    "class MyEnvironment(Environment):\n",
    "    def percepts(self, agent):\n",
    "        ''' the percept is a tuple of (location, status) '''\n",
    "        status = 'Dirty' if any([isinstance(item, Dirt) \n",
    "                                 for item in room.things_at(location=agent.location)]) else 'Clean'\n",
    "        return (agent.location, status)\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        ''' override the default execute_action in Environment class, so that\n",
    "        for the Roomba we remove '''\n",
    "        agent.e_timer += 1\n",
    "        if agent.e_timer == 5:\n",
    "            agent.e_timer = 0 \n",
    "            for i in range(self.width+1):\n",
    "                for j in range(self.height+1):\n",
    "                    p_dirt = 0.0\n",
    "                    updateStatus = np.random.choice(['Clean','Dirty'], p=[1-p_dirt, p_dirt])\n",
    "                    #print('Update status: {}'.format(updateStatus))\n",
    "                    if updateStatus == 'Dirty':\n",
    "                        location = (i, j)\n",
    "                        print('Adding dirt at: {}'.format(location))\n",
    "                        self.add_thing(Dirt(), location)\n",
    "        if action in ['N','S','E','W']:\n",
    "            agent.performance -= 1\n",
    "            bump = self.move(agent, action)\n",
    "        elif action == 'Vacuum':\n",
    "            agent.performance += 5\n",
    "            messes = [thing for thing in room.things_at(roomba.location) if isinstance(thing, Dirt)]\n",
    "            if messes:\n",
    "                agent.holding.append(messes[0])\n",
    "                self.remove_thing(messes[0])\n",
    "        elif action == 'Drop':\n",
    "            if agent.holding:\n",
    "                dropped = agent.holding.pop()\n",
    "                self.add_thing(thing=dropped, location=agent.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randy's performance score: -4\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -3\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -3\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -4\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -3\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -3\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -4\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -3\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -3\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n",
      "Randy's performance score: -4\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n"
     ]
    }
   ],
   "source": [
    "def run_sims(trials, agent):\n",
    "    for i in range(trials):\n",
    "        room = MyEnvironment(width=2, height=1)\n",
    "        roomba = agent\n",
    "        #roomba = ModelRoombaAgent('Randy', None)\n",
    "        roomba = ModelRoombaAgent('Randy', roomba)\n",
    "        \n",
    "        room.add_thing(roomba, location=(0,0))\n",
    "        room.add_thing(Dirt(), location=(0,0))\n",
    "        room.add_thing(Dirt(), location=(1,0))\n",
    "        room.driver(10, True)\n",
    "\n",
    "        print(\"{}'s performance score: {}\".format(roomba.name, roomba.performance))\n",
    "        print('Things left in the room: {}'.format([thing.__class__.__name__ for thing in room.things]))\n",
    "        print('Things agent is holding: {}'.format([thing.__class__.__name__ for thing in roomba.holding]))\n",
    "        \n",
    "run_sims(10, roomba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be a relationship between the agent's performance, the frequency with which new dirt stochastically appears (`p_dirt`), and your `NoOp` action strategy (which avoids excessive moving around).  This might be easiest to think about in terms of the limiting cases:\n",
    "* What should be the agent's `NoOp` strategy if `p_dirt` = 1?\n",
    "* What should be the agent's `NoOp` strategy if `p_dirt` = 0?\n",
    "\n",
    "Note that we don't have any control over `p_dirt`; it is a property of how filthy our living conditions are. All we can do is specify the Roomba's `NoOp` strategy (waiting time before circling back to check for new dirt) as best we can.\n",
    "\n",
    "So, try a few very long simulations (you might want to suppress the printed output for these) with different combinations of `NoOp` strategy and `p_dirt` values.  For a couple different values of `p_dirt`, what do you find to be the optimal `NoOp` strategies? That is, what waiting time (once the agent knows it has cleaned everything) will maximize the agent's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Every three \"environment steps\" seems to output the best performance. However, this is completely dependent on how clean the room is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1c'></a>\n",
    "\n",
    "### (1c)\n",
    "\n",
    "Let's continue to build this up! We can make things a bit more interesting by increasing the size of the room. Add the following features to your codes from **(1b)**:\n",
    "* ~~Try a 2x2 room first.~~\n",
    "* In the example codes, Roomba knew the geography of the environment (i.e., that it is a 2x1 room), so it knew the options for moving around.\n",
    "  * ~~As a first attempt for your 2x2 room, make this assumption.  Thus, the Roomba's choices of action to execute will depend on its sensed location.~~\n",
    "    * Note that if Roomba is at (0,0), and senses that this tile is clean, but has no information about (1,0) or (0,1), then it should choose at random where to go next. A modification of the `np.random.choice` function call above can take care of this.\n",
    "  * Once that seems to work, try to incorporate the fact that the Roomba typically does not know a priori what the shape of the room is. \n",
    "    * The easiest first step to build this into your model is to have Roomba sense its environment: if dirty, then clean the tile; if clean, then move randomly. (This is how I clean my apartment.)\n",
    "    * Then you can try building a **map** of the room as part of the Roomba's model. So as the random-movement Roomba flails around, it will sometimes bump into a wall.  But if it senses a bump, then Roomba can *learn* where the walls are, and what the available actions are depending on where in the room it is.\n",
    "    * Holy heck, we just built a *learning* model-based agent!\n",
    "* Once your Roomba is successfully tidying up a 2x1 or 2x2 room, go bigger!\n",
    "* It might be useful to note that the only thing that really needs to change is your `agent_function`, which maps percepts to actions. The `VacuumEnvironment` class is already able to handle arbitrary-sized rooms. So selecting an action will need to be modified, as well as how the agent builds its internal model of the task environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelRoombaAgent(name, agent):\n",
    "    '''reflex Roomba agent. Takes percept as input, which is a tuple of (location, status)\n",
    "    and returns the appropriate action. This will override the default in the Agent class of \n",
    "    only moving West'''\n",
    "    if agent == None:\n",
    "        return Agent(name, None)\n",
    "    \n",
    "    def agent_function(percepts):\n",
    "        location, status = percepts\n",
    "        '''Timer resets every 3 steps, so we recheck if tiles are clean'''\n",
    "        if agent.timer >= 6:\n",
    "            agent.critical = True\n",
    "            agent.criticalTimer += 1\n",
    "        if agent.timer == 10: #Timer to keep track of how long it's been since we've check for cleanness\n",
    "            agent.clean_tiles = []\n",
    "            agent.timer = 0\n",
    "            \n",
    "        if agent.critical == True or len(agent.holding) >= 2:\n",
    "            locx, locy = location\n",
    "            if 0 > locx:\n",
    "                if room.hit_wall((locx-1, locy)) == False:\n",
    "                    return 'W'\n",
    "            if 0 < locx:\n",
    "                if room.hit_wall((locx+1, locy)) == False:\n",
    "                    return 'E'\n",
    "            if 0 > locy:\n",
    "                if room.hit_wall((locx, locy+1)) == False:\n",
    "                    return 'N'\n",
    "            if 0 < locy:\n",
    "                if room.hit_wall((locx, locy-1)) == False:\n",
    "                    return 'S'\n",
    "            if (0,0) == location:\n",
    "                agent.critical = False\n",
    "                agent.criticalTimer = 0\n",
    "                return 'Drop'\n",
    "            if agent.criticalTimer == 6:\n",
    "                agent.alive = False\n",
    "                print('You lose')\n",
    "                agent.critical = False\n",
    "            \n",
    "        agent.timer += 1\n",
    "        if status == 'Clean':\n",
    "            if location not in agent.clean_tiles:\n",
    "                agent.clean_tiles.append(location)\n",
    "                agent.performance += 5\n",
    "        if status == 'Dirty':\n",
    "            agent.clean_tiles.append(location)\n",
    "            return 'Vacuum'\n",
    "        locx, locy = location\n",
    "        if room.hit_wall((locx-1, locy)) == False: \n",
    "            if(locx-1, locy) not in agent.clean_tiles: \n",
    "                agent.current_moves.append('W')\n",
    "        if room.hit_wall((locx-1, locy)) == True:\n",
    "            if (locx-1, locy) not in agent.walls:\n",
    "                    agent.walls.append((locx-1, locy))\n",
    "        ##################################\n",
    "        if room.hit_wall((locx, locy+1)) == False: \n",
    "            if (locx, locy+1) not in agent.clean_tiles: \n",
    "                agent.current_moves.append('N')\n",
    "        if room.hit_wall((locx, locy+1)) == True:\n",
    "            if (locx, locy+1) not in agent.walls:\n",
    "                    agent.walls.append((locx, locy+1))\n",
    "        ##################################\n",
    "        if room.hit_wall((locx+1, locy)) == False: \n",
    "            if(locx+1, locy) not in agent.clean_tiles: \n",
    "                agent.current_moves.append('E')\n",
    "        if room.hit_wall((locx+1, locy)) == True:\n",
    "            if (locx+1, locy) not in agent.walls:\n",
    "                    agent.walls.append((locx+1, locy))\n",
    "        ##################################\n",
    "        if room.hit_wall((locx, locy-1)) == False: \n",
    "            if(locx, locy-1) not in agent.clean_tiles: \n",
    "                agent.current_moves.append('S')\n",
    "        if room.hit_wall((locx, locy-1)) == True:\n",
    "            if (locx, locy-1) not in agent.walls:\n",
    "                    agent.walls.append((locx, locy-1))\n",
    "        if agent.current_moves == []:\n",
    "            return\n",
    "        choice = np.random.choice(agent.current_moves)\n",
    "        agent.current_moves = []\n",
    "        return choice\n",
    "    return Agent(name, agent_function)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Randy executes action Vacuum at location (0, 0)\n",
      "Agent Randy executes action E at location (0, 0)\n",
      "Agent Randy executes action Vacuum at location (1, 0)\n",
      "Agent Randy executes action N at location (1, 0)\n",
      "Agent Randy executes action Vacuum at location (1, 1)\n",
      "Agent Randy executes action W at location (1, 1)\n",
      "Agent Randy executes action S at location (0, 1)\n",
      "Agent Randy executes action Drop at location (0, 0)\n",
      "Agent Randy executes action Drop at location (0, 0)\n",
      "Agent Randy executes action Drop at location (0, 0)\n",
      "Randy's performance score: 11\n",
      "Things left in the room: ['Agent', 'Dirt', 'Dirt', 'Dirt', 'Dirt']\n",
      "Things agent is holding: []\n"
     ]
    }
   ],
   "source": [
    "room = MyEnvironment(width=2, height=2)\n",
    "roomba = ModelRoombaAgent('Randy', roomba)\n",
    "roomba = ModelRoombaAgent('Randy', roomba)\n",
    "\n",
    "room.add_thing(roomba, location=(0,0))\n",
    "room.add_thing(Dirt(), location=(0,0))\n",
    "room.add_thing(Dirt(), location=(1,0))\n",
    "room.add_thing(Dirt(), location=(0,1))\n",
    "room.add_thing(Dirt(), location=(1,1))\n",
    "room.driver(10)\n",
    "\n",
    "print(\"{}'s performance score: {}\".format(roomba.name, roomba.performance))\n",
    "print('Things left in the room: {}'.format([thing.__class__.__name__ for thing in room.things]))\n",
    "print('Things agent is holding: {}'.format([thing.__class__.__name__ for thing in roomba.holding]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1d'></a>\n",
    "\n",
    "### (1d)\n",
    "\n",
    "Let's add two more related details that will really make this Roomba more realistic.\n",
    "* Roombas have a certain amount of **charge**, which depletes as time goes on. You could incorporate this into the model in a few different ways. Use your ingenuity and creativity to decide how to implement this! If you get stuck, ask other groups or ask me.\n",
    "  * This means that your Roomba is going to need a particular location as a charging spot, and should return to it every so often.\n",
    "  * It is probably reasonable to assume that your Roomba knows how long (how many time steps) it takes to deplete its battery. As part of Roomba's model, it should keep track of how long it will take to return to the charging port and how much battery life remains.\n",
    "  * If the battery falls below a critical point, return to base!\n",
    "  * What should happen if the battery is fully depleted?\n",
    "* Roombas also can only hold a **finite amount of dirt**.\n",
    "  * If the dirt gets \"full\" (you will need to implement some dirt limit as part of the agent), then the Roomba can't vacuum anymore and needs to return to the charging port to dump its dirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "<img src=\"https://i.pinimg.com/736x/ca/0d/9f/ca0d9f851e6c21b737883eecc5083250.jpg\" alt=\"Drawing\" style=\"width: 180px;\"/>\n",
    "\n",
    "\n",
    "<a id='p2'></a>\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "### Decision analysis: A Boulder-ific Example\n",
    "\n",
    "<a id='p2a'></a>\n",
    "### (2a)\n",
    "\n",
    "Suppose a total (including you) of $n=12$ of your friends are planning to have a ski weekend in Steamboat Springs. The condo you will rent costs \\$240 per night, and there is a 3-night minimum stay.  Now suppose all 12 people are positive that they will stay for Friday and Saturday night. But some of the gang are slackers who don't have class until 4 PM on Monday, and they want to stick around for another half-day of skiing.  The decision we are going to analyze is this:  **how should we split up the cost among the 12 friends?**\n",
    "\n",
    "<br>\n",
    "\n",
    "**STOP HERE AND ARGUE AMONGST YOURSELVES BEFORE DOING ANY MATH - HOW DO YOU THINK WE SHOULD SPLIT THE COST?**\n",
    "\n",
    "<br>\n",
    "\n",
    "Have you argued yet? Become emotionally-invested in a stance on this issue before continuing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Okay, now let's proceed.\n",
    "\n",
    "So why do we care?  Well, let's denote by $x$ the ***uncertain*** number of people who will stay for the last night.\n",
    "\n",
    "Suppose $x=12$. Then the obvious choice is to have an even split of the total cost among everyone. That is,\n",
    "\n",
    "$$\\$240 \\times 3\\ \\text{nights} / 12\\ \\text{people} = \\$60\\ \\text{per person}$$\n",
    "\n",
    "That means everyone is paying \\$20/night. Sounds fair!\n",
    "\n",
    "But now what if $x=2$? That is, suppose only two friends stay for Sunday night. Let's keep in mind that there was a 3-night minimum. Now suppose we split the total bill 12 ways, so everyone pays \\$60 total again. Call this the **even split** decision (ES for short). Now under decision $d_{ES}$, the people staying for 2 nights are paying \\$30/night, whereas the 2 slackers who stayed Sunday night as well are only paying \\$20/night. That seems a little bit less fair.\n",
    "\n",
    "As an alternative decision, what about if we split up the cost *per night*. So we divide the Friday night cost among everyone who was in attendance Friday night, and same for Saturday and Sunday. Let's call this the **pro-rated** bill split decision (PR for short).\n",
    "\n",
    "Calculate the **cost per night** for each of the 10 two-night attendeees and for each of the 2 three-night attendees, under the pro-rated decision, $d_{PR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for two-night stayers per night: $16.0, Cost for three-night stayers per night: $56.0\n"
     ]
    }
   ],
   "source": [
    "total_cost = 240\n",
    "cost_fri = 240/3\n",
    "cost_sat = 240/3\n",
    "cost_sun = 240/3\n",
    "\n",
    "cost_per_2 = cost_fri/10 + cost_sat/10\n",
    "cost_per_3 = cost_fri/10 + cost_sat/10 + cost_sun/2\n",
    "print('Cost for two-night stayers per night: ${}, Cost for three-night stayers per night: ${}'.format(cost_per_2, cost_per_3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2b'></a>\n",
    "\n",
    "### (2b)\n",
    "\n",
    "Let's formalize this problem in the context of minimizing a **loss function**.  Once we define a loss function and a prior distribution for the uncertain decision variable $x$ (number of people staying Sunday night), we can pick the decision to minimize the expected loss (i.e., the Bayes' decision). Or we can do something else.  The point is that nothing makes a friend you're arguing with happier than hearing \"hey, let's do some math!\"\n",
    "\n",
    "First, we need to do ourselves a favor and define a function to split the total cost, depending on which decision is made.  Define the function `split(cost_total, d, x, n)` such that:\n",
    "* `cost_total` is the total cost, for all three nights\n",
    "* `d` is the decision ($d_{ES}$ or $d_{PR}$)\n",
    "* `x` is the unknown variable representing the number of people who stay for Sunday night (we assume everyone will stay for Friday and Saturday nights)\n",
    "* `n` is an argument for the total number of people (here, we are assuming `n` is 12, but it's best not to hard-code these kinds of things)\n",
    "* the output of `split` should be a tuple of the cost per night for each of the two-night attendees, and three-night attendees. Call these $C_2$ and $C_3$, for brevity's sake.\n",
    "\n",
    "**Check** that your `split` function returns the expected output when you use $c_{total} = \\$240 \\times 3$, $x=2$, $n=12$, and each of $d = d_{ES}$ and $d=d_{PR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.333333333333336\n",
      "Evenly split: (80.0, 80.0), Pro-Rated: (13.333333333333334, 53.333333333333336)\n"
     ]
    }
   ],
   "source": [
    "def split(cost_total, d, x, n):\n",
    "    if d == 'ES':\n",
    "        c2 = 240/3\n",
    "        c3 = c2\n",
    "        return c2, c3\n",
    "    \n",
    "    if d == 'PR':\n",
    "        cost_per_day = cost_total/3\n",
    "        c2 = (cost_per_day*2)/n\n",
    "        c3 = c2 + cost_per_day/x\n",
    "        print(c3)\n",
    "        return c2, c3\n",
    "    \n",
    "C2 = (split(240, 'ES', 2, 12))\n",
    "c3 = split(240, 'PR', 2, 12)\n",
    "\n",
    "print('Evenly split: {}, Pro-Rated: {}'.format(C2, c3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2c'></a>\n",
    "\n",
    "### (2c)\n",
    "\n",
    "Now for the controversial part: we need to define our loss function.  In decision analyses ranging from splitting the bill for a ski condo to managing a hydroelectric dam to estimating the \"correct\" height by which to augment the levees protecting New Orleans, people are going to disagree about the form of the loss function to assume. We will develop our decision analysis here using one form, but you are encouraged to revise it and try others out to see how the decision you arrive at might be affected by your choice of loss function.\n",
    "\n",
    "*Connection to agents: loss can be viewed inversely as utility (just define utility as $-1 \\times\\ \\text{Loss}$), so the decisions that our Roomba agent, for example, makes could be quite different if it were trying to maximize a different utility function/minimize a different loss function.*\n",
    "\n",
    "In the preamble to this problem, we talked about *fairness*, so perhaps our the loss function we want to minimize should have something to do with equality. Let's have a first attempt by drawing a lesson from linear regresion: let's minimize the sum of squared deviations from a central estimate.\n",
    "\n",
    "So we need (i) a central estimate of cost per night, and (ii) to calculate the (square of) the difference between each person's cost per night, and the central estimate of cost per night.\n",
    "\n",
    "First, define a function to calculate a central estimate of cost per night. What the arguments are will depend on the exact form of this central estimate that you decide to use. Here are some recommendations:\n",
    "* arguments you will definitely need:\n",
    "  * $C_2$, cost per night for the two-night attendees (output from your `split` function)\n",
    "  * $C_3$, cost per night for the three-night attendees (output from your `split` function)\n",
    "* arguments you might need:\n",
    "  * $x$, number of people spending all three nights\n",
    "  * $n$, the total number of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n",
      "[80.0,86.66666666666667] with CE: 76.0\n"
     ]
    }
   ],
   "source": [
    "#C2, C3 = split(240, 12, 2)\n",
    "\n",
    "def central_estimate(cost_total, x, n):\n",
    "    cost_per_day = cost_total / 3\n",
    "    c2 = (cost_per_day/n)*2\n",
    "    print(c2)\n",
    "    z = 0\n",
    "    c3 = c2 + (cost_per_day/(n-x))\n",
    "    return (c3+c2)/2\n",
    "\n",
    "x = central_estimate(240, 12, 2)\n",
    "print(\"[{},{}] with CE: {}\".format(C2, C3, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since this should be a central estimate, any output needs to be bounded between (possibly equal to) $C_2$ and $C_3$. Verify that this is the case for $x=2$ and $n=12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our loss function should be the sum of squared deviations from your central estimate, $\\bar{c}(d,x)$. Note that $\\bar{c}$ depends on the decision $d$ and uncertain decision variable $x$ because those dictate how you split the cost. If we let $c_k(d,x)$ denote the cost per night for person $k$, then the loss is:\n",
    "\n",
    "$$L(d,x) = \\sum_{k=1}^n (c_k(d,x) - \\bar{c}(d,x))^2$$\n",
    "\n",
    "Notice that you can write this without a sum if you note that $c_k(d,x)$ can take only one of two values, $C_2$ and $C_3$.\n",
    "\n",
    "Define a loss function function (that isn't a typo) that takes as arguments:\n",
    "* `cost_total`: the total cost for the three nights\n",
    "* `d`: the decision made\n",
    "* `x`: the uncertain number of people staying all three nights\n",
    "* `n`: the total number of people (all `n` are staying at least Friday and Saturday)\n",
    "\n",
    "and returns the quadratic loss defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.66666666666667\n",
      "13.333333333333334\n"
     ]
    }
   ],
   "source": [
    "def SSE(cost_total, d , x, n, CE):\n",
    "    if d == 'PR':\n",
    "        cost_per_day = cost_total / 3\n",
    "        c2 = (cost_per_day/n)*2\n",
    "       # print(c2)\n",
    "        z = 0\n",
    "        c3 = c2 + (cost_per_day/(n-x))\n",
    "        for i in range(n-x):\n",
    "            z += (c2- CE)**2\n",
    "        for i in range(x):\n",
    "            z += (c3 - CE)**2\n",
    "        #print(z)\n",
    "        z = z / (n-1)\n",
    "        z = np.sqrt(z)\n",
    "        return z\n",
    "    if d == 'ES':\n",
    "        cost_per_day = cost_total/3\n",
    "        z = cost_per_day/n\n",
    "        z = z*3\n",
    "        for i in range(n):\n",
    "            x += (z - CE)**2\n",
    "        x = x / (n-1)\n",
    "        x = np.sqrt(x)\n",
    "        return x\n",
    "    \n",
    "C2, C3 = split(240, 'PR', 12, 2)\n",
    "z = SSE(240, 'ES', 2, 12, central_estimate(240, 2, 12))\n",
    "z = z/11\n",
    "z = np.sqrt(z)\n",
    "#print('Z: ', z)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** that your loss function function returns what you expect it to when you use $x=n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2d'></a>\n",
    "\n",
    "### (2d)\n",
    "\n",
    "Calculate for each possible value of $x$ the loss under the even-split decision, $d_{ES}$. You do not need to include $x=0$, but you should include $x=12$ as a sanity check. Store the result in a list or `numpy` array.  Do the same for the pro-rated split decision, $d_{PR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "13.333333333333334\n",
      "[2.785242495291163, 2.8015147417932384, 2.8176930172383803, 2.8337789310923704, 2.8497740473960578, 2.8656798865402253, 2.8814979269522807, 2.8972296067000594, 2.9128763250176744, 2.9284394437579828, 2.943920288775947, 2.9593201512468612]\n",
      "[4.1778637429367489, 4.1199155309263871, 4.1778637429367489, 4.4282533361843255, 4.972652484064489, 5.9384599116647214, 7.4994949324887363, 9.9453049681289762, 13.856406460551018, 20.608912458625095, 34.367003202067373, 76.009568775612351]\n"
     ]
    }
   ],
   "source": [
    "ES_List = [SSE(240, 'PR', i, 12, central_estimate(240, 2, 12)) for i in range(12)]\n",
    "PR_List = [SSE(240, 'ES', i, 12, central_estimate(240, 2, 12)) for i in range(12)]\n",
    "print(PR_List)\n",
    "print(ES_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot $L(d_{PR},x)$ and $L(d_{ES},x)$ against $x$ on the same set of axes.  Comment on how the loss incurred from each decision is affected by the value of $x$.  Be sure to **label your axes and include a legend**. If we want to hedge against *catastrophic* loss, which decision should we make? That is, which decision minimizes the maximum possible loss?\n",
    "\n",
    "If you have not plotted anything in Python before, there's a couple lines of code to get you off the ground. Of course, you will need to modify it to suit our needs. If you want to learn more about how to customize these plots, recall that at the very beginning of the notebook, we imported `matplotlib.pyplot` as `plt` - so that's what to search for online. And there are **lots** of resources for `pyplot`, because it's a badass plotting package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bac3d2d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(range(len(ES_List)), ES_List, label='label 1')\n",
    "plt.plot(range(len(PR_List)), PR_List, label='label 2')\n",
    "plt.title('Plot of Loss vs. Unknown')\n",
    "plt.xlabel('Change in Unkown # of Sunday Snoozers')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2e'></a>\n",
    "\n",
    "### (2e)\n",
    "\n",
    "We might have enough information based on the plot of **2d** to make a decision, but let's dig one layer deeper. Those decisions we would make are affected, after all, by *what are our beliefs about $x$*?\n",
    "\n",
    "Our beliefs about $x$ can be brought into the mix in the form of the ***prior distribution*** on $x$, $f(x)$. Here, $f(x)$ is discrete, so instead of specifying a probability distribution of some form, we can simply define the probability for each of the possible outcomes ($x=1, x=2, \\ldots, x=12$). And then we just need to make sure they sum to 1 (since that's what a probability distribution is).\n",
    "\n",
    "So the simplest (and often silliest) thing we could do is define **uniform** prior probability to each of the 12 possible outcomes for $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = list(range(1,13))\n",
    "prior = [1]*12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while each probability in `prior` is equal, they are not normalized! So let's normalize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329, 0.083333333333333329]\n"
     ]
    }
   ],
   "source": [
    "norm = np.sum(prior)\n",
    "prior = [prior[i]/norm for i in range(len(prior))]\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the prior probability for each possible value of $x$ as well as the loss under each decision, $L(d_{ES},x)$ and $L(d_{PR},x)$. Use these to calculate the **expected value** of each decision's loss (with respect to $x$). Which decision is the Bayes' Decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3481553119113957, 0.69148160615526133, 1.0396369180666571, 1.408658029415351, 1.8230457364207251, 2.3179173957261185, 2.9428753067668465, 3.7716507207775942, 4.9263512591568457, 6.643760630708937, 9.5076775642145517, 15.84180829551558]\n",
      "[0.0067915491563344923, 0.013488897507040649, 0.020280446663375144, 0.027479029972905872, 0.035562590342721433, 0.045216170469928409, 0.057407374304133917, 0.073574495043765023, 0.096099515340581296, 0.12960143178244207, 0.18546854616532471, 0.30902995325144711]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYYiISFyUMEREJC5KGCIiEpf/D14bf2NTCZrGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bad7919e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expected = 0\n",
    "expected_value = []\n",
    "for i in range(len(prior)):\n",
    "    expected += prior[i]*ES_List[i]\n",
    "    expected_value.append(expected)\n",
    "print(expected_value)\n",
    "normed = [i/sum(expected_value) for i in expected_value]\n",
    "print(normed)\n",
    "\n",
    "plt.plot(range(len(normed)), normed, label='label 2')\n",
    "plt.title('Plot of Loss vs. Unknown')\n",
    "plt.xlabel('Change in Unkown # of Sunday Snoozers')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a uniform prior distribution is a nice first stab at this, but we probably have a better sense of how many people want to stay for Sunday night. At the very least, we should know that it's highly unlikely that $x=1$ or $x=12$. Revise the prior probabilities to reflect our judgment that it is more likely that moderate numbers of people stick around for Sunday night, and less likely that extremal numbers of people stay. As a guide, think about how much more likely it might be for 5 or 6 people (for example) to stay for the third night than it is for only 1 or all 12 to stay. Use this *odds ratio* to guide your selection of the relative prior probabilities, and be sure to check that your discrete prior probability distribution is properly normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this revised \"informative\" prior probability distribution to calculate $E_x[L(d_{ES},x)]$ and $E_x[L(d_{PR},x)]$, and determine what is the Bayes' Decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2f'></a>\n",
    "\n",
    "### (2f)\n",
    "\n",
    "If the decision you make is not strongly affected by changes in the underlying assumptions, then your decision is said to be ***robust***. Was our decision robust against changing the underlying prior distribution $f(x)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are at least **two** other places where making judgments/assumptions could possibly have affected our decision-making? Try changing one of them to see if making other judgments affects the outcome of this decision analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2g'></a>\n",
    "\n",
    "### (2g)\n",
    "\n",
    "#### Expected value of perfect information\n",
    "\n",
    "Unless we make some rather funky assumptions, we know that the **expected value of including uncertainty** (EVIU) is 0 for the quadratic loss  function. (One situation in which it is not is if we were a risk-averse decision-maker, and took as our central estimate for $x$ something besides $E_x[x]$. But we aren't getting into that here.) What about the **expected value of perfect information** regarding $x$, EVPI?\n",
    "\n",
    "Recall that EVPI is defined as\n",
    "\n",
    "$$\\text{EVPI} = E_x[L(d_{Bayes},x)] - E_x[L(d_{pi},x)]$$\n",
    "\n",
    "where $d_{Bayes}$ is the Bayes' Decision and $d_{pi}$ is the decision we would make if we had perfect information about $x$.\n",
    "\n",
    "We know from **2e** what $d_{Bayes}$ is, and the associated expected loss under this decision. Use the one with a more *informative* prior (i.e., not the uniform prior).\n",
    "\n",
    "But what about $d_{pi}$?\n",
    "\n",
    "If we had perfect information about $x$, then we would always pick the decision that gives a lower loss (courtesy of our figure from **2d**). So:\n",
    "\n",
    "$$L(d_{pi},x) = \\min{[L(d_{ES}, x), L(d_{PR}, x)]}$$\n",
    "\n",
    "In other words, for each possible value of $x$, the loss with perfect information is the minimum of the two losses.\n",
    "\n",
    "Use this to calculate $E_x[L(d_{pi},x)]$ and EVPI using the more informative prior.  Then, recalculate EVPI using the uniform prior for $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our certainty/uncertainty regarding $x$ affect the expected value of having perfect information about this quantity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<a id='extra'></a>\n",
    "\n",
    "## Extra cred (not credit)\n",
    "\n",
    "What is the best route from the Engineering Center to the Ralphie statue between Muenzinger and Folsom Stadium?\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1u25gItndw9-BSutPEQI-rNsc1E6wkZav\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
